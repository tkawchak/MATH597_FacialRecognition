{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from skimage import io, transform\n",
    "import random\n",
    "import os\n",
    "import scipy.misc as misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some constants to use\n",
    "BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 256\n",
    "LOG_INTERVAL = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "DROPOUT = 0.2\n",
    "EPOCHS = 1\n",
    "DATASET = \"../sampleData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arguments for training the model\n",
    "#kwargs = {'num_workers': 1, 'pin_memory': True} if args.cuda else {}\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolutional layers\n",
      "input: (3, 224, 224)\n",
      "block1: (64, 112, 112)\n",
      "block2: (64, 56, 56)\n",
      "block3: (128, 28, 28)\n",
      "block4: (256, 14, 14)\n",
      "block5: (512, 7, 7)\n",
      "fcblock: (512, 4, 4)\n",
      "fully connected layers\n",
      "fc1: (8192, 3)\n",
      "output: 3\n"
     ]
    }
   ],
   "source": [
    "# convolution blocks\n",
    "INPUT_SIZE = [224, 224]\n",
    "INPUT_DEPTH = 3\n",
    "BLOCK1_SIZE = (np.array(INPUT_SIZE) / 2).astype(int).tolist()\n",
    "BLOCK1_DEPTH = 64\n",
    "BLOCK2_SIZE = (np.array(BLOCK1_SIZE) / 2).astype(int).tolist()\n",
    "BLOCK2_DEPTH = BLOCK1_DEPTH\n",
    "BLOCK3_SIZE = (np.array(BLOCK2_SIZE) / 2).astype(int).tolist()\n",
    "BLOCK3_DEPTH = BLOCK2_DEPTH * 2\n",
    "BLOCK4_SIZE = (np.array(BLOCK3_SIZE) / 2).astype(int).tolist()\n",
    "BLOCK4_DEPTH = BLOCK3_DEPTH * 2\n",
    "BLOCK5_SIZE = (np.array(BLOCK4_SIZE) / 2).astype(int).tolist()\n",
    "BLOCK5_DEPTH = BLOCK4_DEPTH * 2\n",
    "FC_POOL_SIZE = [4, 4]\n",
    "FC_POOL_DEPTH = BLOCK5_DEPTH\n",
    "\n",
    "# fully connected sizes\n",
    "FC1_SIZE = FC_POOL_SIZE[0]*FC_POOL_SIZE[1]*FC_POOL_DEPTH\n",
    "OUTPUT_SIZE = 3\n",
    "\n",
    "# check the sizes\n",
    "print(\"convolutional layers\")\n",
    "print(\"input: ({0}, {1}, {2})\".format(INPUT_DEPTH, INPUT_SIZE[0], INPUT_SIZE[1]))\n",
    "print(\"block1: ({0}, {1}, {2})\".format(BLOCK1_DEPTH, BLOCK1_SIZE[0], BLOCK1_SIZE[1]))\n",
    "print(\"block2: ({0}, {1}, {2})\".format(BLOCK2_DEPTH, BLOCK2_SIZE[0], BLOCK2_SIZE[1]))\n",
    "print(\"block3: ({0}, {1}, {2})\".format(BLOCK3_DEPTH, BLOCK3_SIZE[0], BLOCK3_SIZE[1]))\n",
    "print(\"block4: ({0}, {1}, {2})\".format(BLOCK4_DEPTH, BLOCK4_SIZE[0], BLOCK4_SIZE[1]))\n",
    "print(\"block5: ({0}, {1}, {2})\".format(BLOCK5_DEPTH, BLOCK5_SIZE[0], BLOCK5_SIZE[1]))\n",
    "print(\"fcblock: ({0}, {1}, {2})\".format(FC_POOL_DEPTH, FC_POOL_SIZE[0], FC_POOL_SIZE[1]))\n",
    "print(\"fully connected layers\")\n",
    "print(\"fc1: ({0}, {1})\".format(FC1_SIZE, OUTPUT_SIZE))\n",
    "print(\"output: {0}\".format(OUTPUT_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "18 Layer residual net model inspired by resnet-18\n",
    "\n",
    "TODO:\n",
    "* Make sure the reshapes (.view()) are correctly applied\n",
    "    * Correct dimensions as each argument (depth, width, height) right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # define a dropout layer\n",
    "        self.drop = nn.Dropout(p=DROPOUT)\n",
    "        \n",
    "        # block1\n",
    "        self.bn0 = nn.BatchNorm2d(INPUT_DEPTH)\n",
    "        self.layer1 = nn.Conv2d(INPUT_DEPTH, BLOCK1_DEPTH, kernel_size=7, stride=2, padding=3)\n",
    "        \n",
    "        # pooling layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # block2\n",
    "        self.bn1 = nn.BatchNorm2d(BLOCK1_DEPTH)\n",
    "        self.layer3 = nn.Conv2d(BLOCK1_DEPTH, BLOCK2_DEPTH, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(BLOCK2_DEPTH) \n",
    "        self.block2 = nn.Conv2d(BLOCK2_DEPTH, BLOCK2_DEPTH, kernel_size=3, padding=1) \n",
    "    \n",
    "        # block3\n",
    "        self.layer7 = nn.Conv2d(BLOCK2_DEPTH, BLOCK3_DEPTH, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(BLOCK3_DEPTH)\n",
    "        self.block3 = nn.Conv2d(BLOCK3_DEPTH, BLOCK3_DEPTH, kernel_size=3, padding=1)\n",
    "        #self.block3_res = nn.Linear(BLOCK2_DEPTH*BLOCK2_SIZE[0]*BLOCK2_SIZE[1], \n",
    "        #                            BLOCK3_DEPTH*BLOCK3_SIZE[0]*BLOCK3_SIZE[1])\n",
    "        \n",
    "        # block4\n",
    "        self.layer11 = nn.Conv2d(BLOCK3_DEPTH, BLOCK4_DEPTH, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn4 = nn.BatchNorm2d(BLOCK4_DEPTH)\n",
    "        self.block4 = nn.Conv2d(BLOCK4_DEPTH, BLOCK4_DEPTH, kernel_size=3, padding=1)\n",
    "        #self.block4_res = nn.Linear(BLOCK3_DEPTH*BLOCK3_SIZE[0]*BLOCK3_SIZE[1], \n",
    "        #                            BLOCK4_DEPTH*BLOCK4_SIZE[0]*BLOCK4_SIZE[1])\n",
    "    \n",
    "        # block5\n",
    "        self.layer15 = nn.Conv2d(BLOCK4_DEPTH, BLOCK5_DEPTH, kernel_size=3, padding=1, stride=2)\n",
    "        self.bn5 = nn.BatchNorm2d(BLOCK5_DEPTH)\n",
    "        self.block5 = nn.Conv2d(BLOCK5_DEPTH, BLOCK5_DEPTH, kernel_size=3, padding=1)\n",
    "        #self.block5_res = nn.Linear(BLOCK4_DEPTH*BLOCK4_SIZE[0]*BLOCK4_SIZE[1], \n",
    "        #                            BLOCK5_DEPTH*BLOCK5_SIZE[0]*BLOCK5_SIZE[1])\n",
    "        \n",
    "        # fully connected\n",
    "        self.bn20 = nn.BatchNorm1d(FC1_SIZE)\n",
    "        self.layer20 = nn.Linear(FC1_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input\n",
    "        f = x\n",
    "        print(f.size())\n",
    "        \n",
    "        # block 1\n",
    "        f = self.layer1(F.relu(self.bn0(self.drop(f))))\n",
    "        print(f.size())\n",
    "        \n",
    "        # pool\n",
    "        fres = self.pool(f)\n",
    "        print(fres.size())\n",
    "        \n",
    "        # block 2\n",
    "        f = self.layer3(F.relu(self.bn1(self.drop(fres))))\n",
    "        fres = self.block2(F.relu(self.bn2(self.drop(f)))) + fres\n",
    "        f = self.block2(F.relu(self.bn2(self.drop(fres))))\n",
    "        fres = self.block2(F.relu(self.bn2(self.drop(f)))) + fres\n",
    "        print(fres.size())\n",
    "        \n",
    "        # block 3\n",
    "        f = self.layer7(F.relu(self.bn2(self.drop(fres))))\n",
    "        fres = self.block3(F.relu(self.bn3(self.drop(f)))) #+ \\\n",
    "            #self.block3_res(fres.view(-1, BLOCK2_DEPTH*BLOCK2_SIZE[0]*BLOCK2_SIZE[1]))\\\n",
    "            #.view(BATCH_SIZE, BLOCK3_DEPTH, BLOCK3_SIZE[0], BLOCK3_SIZE[1])\n",
    "        f = self.block3(F.relu(self.bn3(self.drop(fres))))\n",
    "        fres = self.block3(F.relu(self.bn3(self.drop(f)))) + fres\n",
    "        print(fres.size())\n",
    "        \n",
    "        # block 4\n",
    "        f = self.layer11(F.relu(self.bn3(self.drop(fres))))\n",
    "        fres = self.block4(F.relu(self.bn4(self.drop(f)))) #+ \\\n",
    "            #self.block4_res(fres.view(-1, BLOCK3_DEPTH*BLOCK3_SIZE[0]*BLOCK3_SIZE[1]))\\\n",
    "            #.view(BATCH_SIZE, BLOCK4_DEPTH, BLOCK4_SIZE[0], BLOCK4_SIZE[1])\n",
    "        f = self.block4(F.relu(self.bn4(self.drop(fres))))\n",
    "        fres = self.block4(F.relu(self.bn4(self.drop(f)))) + fres\n",
    "        print(fres.size())\n",
    "        \n",
    "        # block 5\n",
    "        f = self.layer15(F.relu(self.bn4(self.drop(fres))))\n",
    "        f = self.block5(F.relu(self.bn5(self.drop(f)))) #+ \\\n",
    "            #self.block5_res(fres.view(-1, BLOCK4_DEPTH*BLOCK4_SIZE[0]*BLOCK4_SIZE[1]))\\\n",
    "            #.view(BATCH_SIZE, BLOCK5_DEPTH, BLOCK5_SIZE[0], BLOCK5_SIZE[1])\n",
    "        f = self.block5(F.relu(self.bn5(self.drop(f))))\n",
    "        f = self.block5(F.relu(self.bn5(self.drop(f))))\n",
    "        print(f.size())\n",
    "        \n",
    "        # pool \n",
    "        f = self.pool(f)\n",
    "        print(f.size())\n",
    "        \n",
    "        # fc\n",
    "        print(f.size())\n",
    "        print(f.view(BATCH_SIZE, FC1_SIZE).size())\n",
    "        f = self.layer20(F.relu(self.bn20(f.view(BATCH_SIZE, FC1_SIZE))))\n",
    "        print(f.size())\n",
    "        \n",
    "        # return the softmax of the probability\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "\n",
    "# if we want to use gpu: \n",
    "#model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the data loaders\n",
    "Load in the training data and test data from batches\n",
    "\n",
    "TODO:\n",
    "* configure the correct batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MegaFaceDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    All datasets are subclasses of torch.utils.data.Dataset i.e, they have\n",
    "    __getitem__ and __len__ methods implemented.\n",
    "    Hence, they can all be passed to a torch.utils.data.DataLoader which can\n",
    "    load multiple samples parallelly using torch.multiprocessing workers\n",
    "\n",
    "    Source: http://pytorch.org/docs/master/torchvision/datasets.html\n",
    "\n",
    "    If you want to use \"ImageFolder\", the data in the root folder must be\n",
    "    arranged in this way:\n",
    "\n",
    "    root/dog/xxx.png\n",
    "    root/dog/xxy.png\n",
    "    root/dog/xxz.png\n",
    "\n",
    "    root/cat/123.png\n",
    "    root/cat/nsdf3.png\n",
    "    root/cat/asd932_.png\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform = None):\n",
    "        self.transform = transform #tranform the input (image augmentation)\n",
    "        self.root_dir = root_dir\n",
    "        self.image_names = os.listdir(root_dir)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"__getitem__ supports the indexing such that dataset[i] can be used to\n",
    "        get the ith sample\"\"\"\n",
    "\n",
    "        img_name = os.path.join(self.root_dir, self.image_names[index])\n",
    "        image = np.array(Image.open(img_name))\n",
    "        label = int(img_name.split(\"_\")[1].split(\".\")[0]) - 1\n",
    "        #print(\"index: {2}, img_name: {0}, label: {1}\".format(img_name, label, index+1))\n",
    "        oh_label = np.zeros(OUTPUT_SIZE, dtype=int)\n",
    "        oh_label[label] = 1\n",
    "        #print(\"image shape: {0}, label shape: {1}\".format(image.shape, oh_label.shape))\n",
    "        sample = {'image': image, 'label': oh_label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            # image = self.transform(image)\n",
    "            # label = self.transform(label)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"__len__ returns the size of the dataset. Use by calling len(dataset)\"\"\"\n",
    "        return len(self.image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        image = misc.imresize(image, (INPUT_SIZE[0], INPUT_SIZE[1]))\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        #print(image.shape)\n",
    "        #print(\"input size: ({0}, {1}, {2})\".format(INPUT_DEPTH, INPUT_SIZE[0], INPUT_SIZE[1]))\n",
    "        image = torch.from_numpy(image).float()\n",
    "        label = torch.from_numpy(label).long()\n",
    "        sample = {'image': image, 'label': label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "megaface_dataset = MegaFaceDataset(root_dir=DATASET,\n",
    "                                        transform=ToTensor())\n",
    "\n",
    "train_loader = DataLoader(megaface_dataset, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i in train_loader:\n",
    "#    print(\"loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform = transforms.Compose(\n",
    "#    [transforms.ToTensor()])\n",
    "\n",
    "#trainset = torchvision.datasets.STL10(root='./data', split='train',\n",
    "#                                        download=True, transform=transform)\n",
    "#train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "#                                          shuffle=True, num_workers=2)\n",
    "\n",
    "#testset = torchvision.datasets.STL10(root='./data', split='test',\n",
    "#                                       download=True, transform=transform)\n",
    "#test_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "#                                         shuffle=False, num_workers=2)\n",
    "\n",
    "#classes = ('plane', 'car', 'bird', 'cat',\n",
    "#           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor()\n",
    "#                    ])),\n",
    "#     batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "#                        transforms.ToTensor()\n",
    "#                    ])),\n",
    "# batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "# print out the data to check\n",
    "#for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer\n",
    "\"\"\"\n",
    "params (iterable) – iterable of parameters to optimize or dicts defining parameter groups\n",
    "lr (float, optional) – learning rate (default: 1e-3)\n",
    "betas (Tuple[float, float], optional) – coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\n",
    "eps (float, optional) – term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)\n",
    "\"\"\"\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifiy what training will take place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define training function\n",
    "def train(epoch, model):\n",
    "    \"\"\"\n",
    "        Train the model\n",
    "        Inputs:\n",
    "            epoch - number of the current epoch\n",
    "            \n",
    "        Outputs:\n",
    "            \n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        print(\"batch index: {0}, data: {1}, target: {2}\".format(batch_idx, data['image'].shape, data['label'].shape))\n",
    "        image, target = Variable(data['image']), Variable(data['label'])\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How will we test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    \"\"\"\n",
    "        Test the model's accuracy\n",
    "        Inputs:\n",
    "            None\n",
    "        Outputs: \n",
    "            Prints the test output results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Training & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch index: 0, data: torch.Size([4, 3, 224, 224]), target: torch.Size([4, 3])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 64, 112, 112])\n",
      "torch.Size([4, 64, 56, 56])\n",
      "torch.Size([4, 64, 56, 56])\n",
      "torch.Size([4, 128, 28, 28])\n",
      "torch.Size([4, 256, 14, 14])\n",
      "torch.Size([4, 512, 7, 7])\n",
      "torch.Size([4, 512, 4, 4])\n",
      "torch.Size([4, 512, 4, 4])\n",
      "torch.Size([4, 8192])\n",
      "torch.Size([4, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 2 at /opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THNN/generic/SpatialClassNLLCriterion.c:40",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ec1537c70689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-4a75f3076540>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/facialRecognition/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index)\u001b[0m\n\u001b[1;32m    672\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2 or 4 dimensions (got {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/facialRecognition/lib/python3.6/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, target, *args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, target,\n\u001b[0;32m---> 47\u001b[0;31m                                                   output, *ctx.additional_args)\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 2 at /opt/conda/conda-bld/pytorch_1503965122592/work/torch/lib/THNN/generic/SpatialClassNLLCriterion.c:40"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS+1):\n",
    "    train(epoch, model)\n",
    "    test(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Some extra cells to print testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x7f21f2de95c8>\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (facialRecognition)",
   "language": "python",
   "name": "facialrecognition"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
